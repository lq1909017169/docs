---
title: Quickstart
---

import AlphaCallout from '/snippets/alpha-lc-callout.mdx';

<AlphaCallout />

This quickstart will take you from zero to a fully functional AI agent in just a few minutes. You will start simple and gradually build up to a more sophisticated agent.

## Build a basic agent

Let's begin with agent basics - creating a simple agent that can answer questions and use tools. Create an agent with the following characteristics:

- A language model (Claude 3.7 Sonnet)
- A simple tool (weather function)
- A basic prompt
- The ability to invoke it with messages

:::python
```python
from langchain.agents import create_agent

def get_weather(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

agent = create_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[get_weather],
    prompt="You are a helpful assistant",
)

# Run the agent
agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)
```
:::

:::js
```ts
import { createAgent, tool } from "langchain";

const getWeather = tool((city: string) => `It's always sunny in ${city}!`, {
    name: "get_weather",
    description: "Get the weather for a given city",
});

const agent = createAgent({
    model: "anthropic:claude-3-7-sonnet-latest",
    tools: [getWeather],
});

console.log(
    await agent.invoke({
        messages: [{ role: "user", content: "What's the weather in Tokyo?" }],
    })
);
```
:::

## Build a real-world agent

Now let's create something more practical. Let's build a weather forecasting agent that demonstrates the key concepts you would use in production:

1. **Detailed system prompts** for better agent behavior
2. **Real-world tools** that integrate with external data
3. **Model configuration** for consistent responses
4. **Structured output** for predictable results
5. **Conversational memory** for chat-like interactions
6. **Bring it all together** to create a fully functional agent

Let's walk through each step:

<Steps>
    <Step title="Define the system prompt">
        The system prompt is your agent's personality and instructions. Make it
        specific and actionable:

        :::python
        ```python wrap
        system_prompt = """You are an expert weather forecaster, who speaks in puns.

        You have access to two tools:

        - get_weather_for_location: use this to get the weather for a specific location
        - get_user_location: use this to get the user's location

        If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean whereever they are, use the get_user_location tool to find their location."""
        ```
        :::

        :::js
        ```ts
        const systemPrompt = `You are an expert weather forecaster, who speaks in puns.

        You have access to two tools:

        - get_weather_for_location: use this to get the weather for a specific location
        - get_user_location: use this to get the user's location

        If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean whereever they are, use the get_user_location tool to find their location.`;
        ```
        :::
    </Step>
    <Step title="Create tools">
        :::python
        [Tools](/oss/langchain/tools) are functions your agent can call. They should be well-documented. Oftentimes, tools will want to connect to external systems, and will rely on runtime configuration to do so. Notice here how the `get_user_location` tool does exactly that:

        ```python
        from langchain_core.tools import tool

        def get_weather_for_location(city: str) -> str:  # (1)!
            """Get weather for a given city."""
            return f"It's always sunny in {city}!"

        from langchain_core.runnables import RunnableConfig

        USER_LOCATION = {
            "1": "Florida",
            "2": "SF"
        }

        @tool
        def get_user_location(config: RunnableConfig) -> str:
            """Retrieve user information based on user ID."""
            user_id = config["context"].get("user_id")
            return USER_LOCATION[user_id]
        ```
        :::

        :::js
        [Tools](/oss/langchain/tools) are functions your agent can call. They should be well-documented. Oftentimes tools will want to connect to external systems, and will rely on runtime configuration to do so. Notice here how the `getUserLocation` tool does exactly that:

        ```ts
        import { tool } from "langchain";
        import { z } from "zod";

        const getWeather = tool(({ city }) => `It's always sunny in ${city}!`, {
            name: "get_weather_for_location",
            description: "Get the weather for a given city",
            schema: z.object({
                city: z.string(),
            }),
        });

        const USER_LOCATION = {
            "1": "Florida",
            "2": "SF",
        } as const;

        const getUserLocation = tool(
            (_, config) => {
                const { user_id } = config.context as {
                    user_id: keyof typeof USER_LOCATION;
                };
                console.log("user_id", config.context);
                return USER_LOCATION[user_id];
            },
            {
                name: "get_user_location",
                description: "Retrieve user information based on user ID",
                schema: z.object({}),
            }
        );
        ```

        <Note>
            [Zod](https://zod.dev/) is a library for validating and parsing pre-defined schemas. You can use it to define the input schema for your tools to make sure the agent only calls the tool with the correct arguments.

            Alternatively, you can define the `schema` property as a JSON schema object. Keep in mind that JSON schemas *won't* be validated at runtime.

            <Accordion title="Example: Using JSON schema for tool input">
                ```ts
                const getWeather = tool(
                    ({ city }) => `It's always sunny in ${city}!`,
                {
                    name: "get_weather_for_location",
                    description: "Get the weather for a given city",
                    schema: {
                    type: "object",
                    properties: {
                        city: {
                            type: "string",
                            description: "The city to get the weather for"
                        }
                    },
                    required: ["city"]
                    },
                });
            ```
            </Accordion>
        </Note>
        :::
    </Step>
    <Step title="Configure your model">
        Set up your language model with the right parameters for your use case:

        :::python
        ```python
        from langchain.chat_models import init_chat_model

        model = init_chat_model(
            "anthropic:claude-3-7-sonnet-latest",
            temperature=0
        )
        ```
        :::

        :::js
        ```ts
        import { initChatModel } from "langchain/chat_models";

        const model = await initChatModel(
            "anthropic:claude-3-7-sonnet-latest",
            temperature: 0
        );
        ```
        :::
    </Step>
    <Step title="Define response format">
        :::python
        Structured outputs ensure your agent returns data in a predictable
        format. Here, we use Python's [`DataClass`](https://docs.python.org/3/library/dataclasses.html)
        dictionary.

        ```python
        from dataclasses import dataclass

        @dataclass
        class WeatherResponse:
            conditions: str
            punny_response: str
        ```
        :::

        :::js
        Structured outputs ensure your agent returns data in a predictable format.

        ```ts
        const responseFormat = z.object({
            conditions: z.string(),
            punny_response: z.string(),
        });
        ```
        :::
    </Step>
    <Step title="Add memory">
        Enable your agent to remember conversation history:

        :::python
        ```python
        from langgraph.checkpoint.memory import InMemorySaver

        checkpointer = InMemorySaver()
        ```
        :::

        :::js
        ```ts
        import { MemorySaver } from "langchain";

        const checkpointer = new MemorySaver();
        ```
        :::
    </Step>
    <Step title="Bring it all together">
        Now assemble your agent with all the components:

        :::python
        ```python
        agent = create_agent(
            model=model,
            prompt=system_prompt,
            tools=[get_user_location, get_weather_for_location],
            response_format=WeatherResponse,
            checkpointer=checkpointer
        )

        config = {"configurable": {"thread_id": "1"}}
        context = {"user_id": "1"}
        response = agent.invoke(
            {"messages": [{"role": "user", "content": "what is the weather outside?"}]},
            config=config,
            context=context
        )

        response['structured_response']

        response = agent.invoke(
            {"messages": [{"role": "user", "content": "thank you!"}]},
            config=config,
            context=context
        )

        response['structured_response']
        ```
        :::

        :::js
        ```ts
        import { createAgent } from "langchain";

        const agent = createAgent({
            model: "anthropic:claude-3-7-sonnet-latest",
            prompt: systemPrompt,
            tools: [getUserLocation, getWeather],
            responseFormat,
            checkpointer,
        });

        const config = {
            configurable: { thread_id: "1" },
            context: { user_id: "1" },
        };
        const response = await agent.invoke(
            { messages: [{ role: "user", content: "what is the weather outside?" }] },
            config
        );
        console.log(response.structuredResponse);

        const thankYouResponse = await agent.invoke(
            { messages: [{ role: "user", content: "thank you!" }] },
            config
        );
        console.log(thankYouResponse.structuredResponse);
        ```
        :::
    </Step>
</Steps>

Congratulations! You now have a sophisticated AI agent that can:

- **Understand context** and remember conversations
- **Use multiple tools** intelligently
- **Provide structured responses** in a consistent format
- **Handle user-specific information** through context
- **Maintain conversation state** across interactions
